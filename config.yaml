model:
  n_layers: 16            # Number of layers in the model architecture
  d_model: 768            # Model dimension
  n_heads: 12             # Number of attention heads
  n_kv_heads: 2           # GQA: KV heads < Q heads
  d_ff: 3072              # ~4x d_model, SwiGLU inside will split
  dropout: 0.0            # Dropping out tokens to prevent overfitting, specify percentage (0.0 to disable)
  vocab_size: null        # filled from tokenizer
  rope_theta: 1000000     # RoPE base; larger supports longer context
  tie_embeddings: true    # Tie embeddings to prevent overfitting
  swa_window: 0           # set >0 to enable Sliding-Window Attention (tokens can attend last W)

training:
  seq_len: 1024           # Sequence length
  micro_batch_size: 1     # Micro batch size
  grad_accum_steps: 32    # effective batch = micro_batch * accum * devices
  max_steps: 3000         # Maximum number of training steps
  eval_every: 200         # not used when steps_per_epoch is set; kept for reference
  save_every: 200         # Save every X steps
  lr: 2e-4                # learning rate, it's rate of change of loss with respect to parameters, 3e-4 is default  
  weight_decay: 0.1       # it's a regularization term that prevents overfitting, 0.1 is default
  betas: [0.9, 0.95]      # it's a hyperparameter which scale and shift normalized input
  eps: 1.0e-8             # it's a hyperparameter which prevents division by zero
  warmup_ratio: 0.01      # it's a hyperparameter which controls the amount of warmup steps
  precision: "32"         # it's a hyperparameter which controls the precision of the optimizer
  seed: 42                # it's a hyperparameter which controls the random seed
  steps_per_epoch: 200    # align epoch length with desired validation cadence

  # Learning-rate reduction on plateau (val_loss)
  reduce_on_plateau:
    factor: 0.5           # Factor by which the learning rate will be reduced
    patience: 2           # Number of epochs with no improvement after which learning rate will be reduced
    threshold: 0.0        # Threshold for measuring the new optimum, to only focus on significant changes
    cooldown: 0           # Number of epochs to wait before resuming normal operation after lr has been reduced
    min_lr: 1.0e-6        # Minimum learning rate

  # Early stopping on validation loss
  early_stopping:
    patience: 5           # Number of epochs with no improvement after which training will be stopped
    min_delta: 0.0        # Minimum change in the monitored quantity to qualify as an improvement

hardware:
  accelerator: "mps"      # "mps" on Apple Silicon; use "gpu" on 4090; "auto" also works
  devices: 1
  num_workers: 2

# Dataset options. Start with a preprocessed, manageable corpus.
data:
  dataset: "shahrukhx01/wikipedia-bookscorpus-en-preprocessed"
  split: "train"
  text_field: "text"
  streaming: false        # memory-mapped Arrow; set true for huge corpora
  tokenizer_name: "gpt2"  # change to a Llama tokenizer later if you have access
  max_shards: null        # optional: limit shards/rows for quick smoke tests
  pack_sequences: true    # concatenate tokens and pack into fixed-length blocks

  # Dataset size configuration
  train_docs: 20000       # Number of training documents
  val_docs: 2000          # Number of validation documents (separate batch)
